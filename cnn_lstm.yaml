#data_pathsc
train_path: '/home/riseadmin/cnn_dm_small/chunked/train*'
dev_path: '/home/riseadmin/cnn_dm_small/chunked/val*'
test_path: '/home/riseadmin/cnn_dm_small/chunked/test*'
vocab_path: '/home/riseadmin/cnn_dm_small/vocab'
glove_path: '/home/riseadmin/glove/glove.6B.100d.txt'

#glove options
use_glove: False
emb_trainable: True
optimizer: adagrad
#output paths
log_root: '/home/riseadmin/gttp_logs' #Root directory for all logging.
base_experiment: 'gcn_cnn'
exp_name: 'gcn_lstm' #Name for experiment. Logs will be saved in a directory with this name, under log_root
tf_example_format: True
#Hyperparameters
hidden_dim: 128 #dimension of RNN hidden states
emb_dim: 128 #dimension of word embeddings
batch_size: 16  #minibatch size
max_enc_steps: 400 #max timesteps of encoder (max source text tokens)
max_query_steps: 100 #max timesteps of query encoder (max source query tokens)
max_dec_steps: 100  #max timesteps of decoder (max summary tokens)
beam_size: 4  #beam size for beam search decoding.
min_dec_steps: 3 #Minimum sequence length of generated summary. Applies only for beam search decoding mode
vocab_size: 50000  #'Size of vocabulary. These will be read from the vocabulary file in order. If the vocabulary file contains fewer words than this number, or if this number is set to 0, will take all words in the vocabulary file.
lr: 0.015 #learning rate
adam_lr: 0.0004
adagrad_init_acc: 0.01 #initial accumulator value for Adagrad
rand_unif_init_mag: 0.02 #magnitude for lstm cells random uniform inititalization
trunc_norm_init_std: 1e-4 #std of trunc norm init, used for initializing everything else
max_grad_norm: 2.0 #for gradient clipping

#other_options
pointer_gen: True #If True, use pointer-generator model. If False, use baseline model.
query_encoder: False #cnn dm
max_to_keep: 7
gpu_device_id: '1'

#stop_after
use_stop_after: True
stop_steps: 30000

#save_at
use_save_at: False
save_steps: 50

use_lstm: True
use_gcn_lstm_parallel: False
use_label_information: True
concat_gcn_lstm: False

#word gcn
no_lstm_encoder: False #Removes LSTM layer from the seq2seq model. word_gcn flag should be true.
word_gcn: True #If True, use pointer-generator with gcn at word level. If False, use other options.
word_gcn_gating: False #If True, use gating at word level
word_gcn_dropout: 1.0 #dropout keep probability for the gcn layer
word_gcn_layers: 1 #Layers at gcn
word_gcn_dim: 128 #output of gcn 

#query gcn
no_lstm_query_encoder: False #Removes LSTM layer from the seq2seq model. word_gcn flag should be true.
query_gcn: False #If True, use pointer-generator with gcn at query level. If False, use other options.
query_gcn_gating: False #If True, use gating at word level
query_gcn_dropout: 1.0 #dropout keep probability for the gcn layer
query_gcn_layers: 1 #Layers at gcn
query_gcn_dim: 128 #output of gcn query_

#hidden_state_representation: ''#lstm_gcn, gcn_lstm, concat_lstm_gcn, concat_gcn_lstm, gcn_w_lstm
