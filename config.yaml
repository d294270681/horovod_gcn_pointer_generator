#data_paths
train_path: '/home/ubuntu/preksha_abs_summ/train*.pkl'
dev_path: '/home/ubuntu/preksha_abs_summ/val*'
test_path: '/home/ubuntu/preksha_abs_summ/test*'
vocab_path: '/home/ubuntu/preksha_abs_summ/finished_files/vocab'

#output paths
log_root: '/home/ubuntu/gttp_logs' #Root directory for all logging.
exp_name: 'q_test' #Name for experiment. Logs will be saved in a directory with this name, under log_root
#Hyperparameters
hidden_dim: 256 #dimension of RNN hidden states
emb_dim: 100 #dimension of word embeddings
batch_size: 16  #minibatch size
max_enc_steps: 500 #max timesteps of encoder (max source text tokens)
max_query_steps: 100 #max timesteps of query encoder (max source query tokens)
max_dec_steps: 30 #max timesteps of decoder (max summary tokens)
beam_size: 4  #beam size for beam search decoding.
min_dec_steps: 3 #Minimum sequence length of generated summary. Applies only for beam search decoding mode
vocab_size: 25000  #'Size of vocabulary. These will be read from the vocabulary file in order. If the vocabulary file contains fewer words than this number, or if this number is set to 0, will take all words in the vocabulary file.
lr: 0.15 #learning rate
adagrad_init_acc: 0.01 #initial accumulator value for Adagrad
rand_unif_init_mag: 0.02 #magnitude for lstm cells random uniform inititalization
trunc_norm_init_std: 1e-4 #std of trunc norm init, used for initializing everything else
max_grad_norm: 2.0 #for gradient clipping

#other_options
pointer_gen: True #If True, use pointer-generator model. If False, use baseline model.
query_encoder: False
max_to_keep: 3
gpu_device_id: '0'


#word gcn
no_lstm_encoder: False #Removes LSTM layer from the seq2seq model. word_gcn flag should be true.
word_gcn: True #If True, use pointer-generator with gcn at word level. If False, use other options.
word_gcn_gating: False #If True, use gating at word level
word_gcn_dropout: 1.0 #dropout keep probability for the gcn layer
word_gcn_layers: 1 #Layers at gcn
word_gcn_dim: 256 #output of gcn 

#query gcn
no_lstm_query_encoder: False #Removes LSTM layer from the seq2seq model. word_gcn flag should be true.
query_gcn: False #If True, use pointer-generator with gcn at query level. If False, use other options.
query_gcn_gating: False #If True, use gating at word level
query_gcn_dropout: 1.0 #dropout keep probability for the gcn layer
query_gcn_layers: 1 #Layers at gcn
query_gcn_dim: 256 #output of gcn query_

