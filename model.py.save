# Copyright 2016 The TensorFlow Authors. All Rights Reserved.
# Modifications Copyright 2017 Abigail See
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""This file contains code to build and run the tensorflow graph for the sequence-to-sequence model"""

import os
import time
import numpy as np
import tensorflow as tf
from attention_decoder import attention_decoder
from tensorflow.contrib.tensorboard.plugins import projector

FLAGS = tf.app.flags.FLAGS

class SummarizationModel(object):
  """A class to represent a sequence-to-sequence model for text summarization. Supports both baseline mode, pointer-generator mode, and coverage"""

  def __init__(self, hps, vocab):
    self._hps = hps
    self._vocab = vocab
    self.regularizer = None

  def _add_placeholders(self):
    """Add placeholders to the graph. These are entry points for any input data."""
    hps = self._hps

    # encoder part
    self._enc_batch = tf.placeholder(tf.int32, [hps.batch_size, None], name='enc_batch')
    self._enc_lens = tf.placeholder(tf.int32, [hps.batch_size], name='enc_lens')
    self._enc_padding_mask = tf.placeholder(tf.float32, [hps.batch_size, None], name='enc_padding_mask')
    if FLAGS.pointer_gen:
      self._enc_batch_extend_vocab = tf.placeholder(tf.int32, [hps.batch_size, None], name='enc_batch_extend_vocab')
      self._max_art_oovs = tf.placeholder(tf.int32, [], name='max_art_oovs')
    
    if FLAGS.word_gcn:
      #tf.logging.info(hps.num_word_dependency_labels)
      self._word_adj_in  = [{lbl: tf.sparse_placeholder(tf.float32,  shape=[None, None],  name='word_adj_in_{}'.  format(lbl)) for lbl in range(hps.num_word_dependency_labels)} for _ in range(hps.batch_size)]
      self._word_adj_out = [{lbl: tf.sparse_placeholder(tf.float32,  shape=[None, None],  name='word_adj_out_{}'. format(lbl)) for lbl in range(hps.num_word_dependency_labels)} for _ in range(hps.batch_size)]  
      self._word_gcn_dropout = tf.placeholder_with_default(hps.word_gcn_dropout,     shape=(), name='dropout')
      self._max_word_seq_len = tf.placeholder(tf.int32, shape=(), name='max_seq_len')
    # decoder part
    self._dec_batch = tf.placeholder(tf.int32, [hps.batch_size, hps.max_dec_steps], name='dec_batch')
    self._target_batch = tf.placeholder(tf.int32, [hps.batch_size, hps.max_dec_steps], name='target_batch')
    self._dec_padding_mask = tf.placeholder(tf.float32, [hps.batch_size, hps.max_dec_steps], name='dec_padding_mask')

    if hps.mode=="decode" and hps.coverage:
      self.prev_coverage = tf.placeholder(tf.float32, [hps.batch_size, None], name='prev_coverage')


  def _make_feed_dict(self, batch, just_enc=False):
    """Make a feed dictionary mapping parts of the batch to the appropriate placeholders.

    Args:
      batch: Batch object
      just_enc: Boolean. If True, only feed the parts needed for the encoder.
    """
    hps = self._hps
    feed_dict = {}
    feed_dict[self._enc_batch] = batch.enc_batch
    feed_dict[self._enc_lens] = batch.enc_lens
    feed_dict[self._enc_padding_mask] = batch.enc_padding_mask
    if FLAGS.pointer_gen:
      feed_dict[self._enc_batch_extend_vocab] = batch.enc_batch_extend_vocab
      feed_dict[self._max_art_oovs] = batch.max_art_oovs
    
    if FLAGS.word_gcn:
      word_adj_in = batch.word_adj_in
      word_adj_out = batch.word_adj_out
      for i in range(hps.batch_size):
        for lbl in range(hps.num_word_dependency_labels):
          feed_dict[self._word_adj_in[i][lbl]] = tf.SparseTensorValue(  indices   = np.array([word_adj_in[i][lbl].row, word_adj_in[i][lbl].col]).T,
                                values    = word_adj_in[i][lbl].data,
                          dense_shape = word_adj_in[i][lbl].shape)

          feed_dict[self._word_adj_out[i][lbl]] = tf.SparseTensorValue(  indices  = np.array([word_adj_out[i][lbl].row, word_adj_out[i][lbl].col]).T,
                              values    = word_adj_out[i][lbl].data,
                          dense_shape = word_adj_out[i][lbl].shape)

      '''

      feed_dict[self._word_adj_in] = batch.word_adj_in
      feed_dict[self._word_adj_out] = batch.word_adj_out
      '''
    if not just_enc:
      feed_dict[self._dec_batch] = batch.dec_batch
      feed_dict[self._target_batch] = batch.target_batch
      feed_dict[self._dec_padding_mask] = batch.dec_padding_mask
    return feed_dict

  def _add_encoder(self, encoder_inputs, seq_len):
    """Add a single-layer bidirectional LSTM encoder to the graph.

    Args:
      encoder_inputs: A tensor of shape [batch_size, <=max_enc_steps, emb_size].
      seq_len: Lengths of encoder_inputs (before padding). A tensor of shape [batch_size].

    Returns:
      encoder_outputs:
        A tensor of shape [batch_size, <=max_enc_steps, 2*hidden_dim]. It's 2*hidden_dim because it's the concatenation of the forwards and backwards states.
      fw_state, bw_state:
        Each are LSTMStateTuples of shape ([batch_size,hidden_dim],[batch_size,hidden_dim])
    """
    with tf.variable_scope('encoder'):
      cell_fw = tf.contrib.rnn.LSTMCell(self._hps.hidden_dim, initializer=self.rand_unif_init, state_is_tuple=True)
      cell_bw = tf.contrib.rnn.LSTMCell(self._hps.hidden_dim, initializer=self.rand_unif_init, state_is_tuple=True)
      (encoder_outputs, (fw_st, bw_st)) = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, encoder_inputs, dtype=tf.float32, sequence_length=seq_len, swap_memory=True)
      encoder_outputs = tf.concat(axis=2, values=encoder_outputs) # concatenate the forwards and backwards states
    return encoder_outputs, fw_st, bw_st



  def _add_gcn_layer(self, gcn_in,  in_dim, gcn_dim, batch_size, max_nodes, max_labels, adj_in, adj_out, num_layers=1, use_gating=False, dropout=1.0, name="GCN"):
    
    """ Adds GCN layers to the graph. This is a directed GCN
    
    Args:
     gcn_in: Input to GCN Layer
     in_dim: Dimension of input to GCN Layer 
     gcn_dim: Hidden state dimension of GCN
     batch_size: Batch size
     max_nodes: Maximum number of nodes in graph
     max_labels: Maximum number of edge labels
     adj_in: Adjacency matrix for in edges
     adj_out: Adjacency matrix for out edges
     num_layers: Number of GCN Layers
     use_gating: Edge level gating if True
     dropout: Rate of dropout within the layer
    Returns
     out: Outputs from GCN layers. Appended at the index is the final output         
    """

    out = []
    out.append(gcn_in)
#    return tf.nn.relu(tf.zeros([batch_size, max_nodes, gcn_dim]))      	
    for layer in range(num_layers):
      gcn_in    = out[-1]           # out contains the output of all the GCN layers, intitally contains input to first GCN Layer
      if len(out) > 1: in_dim = gcn_dim         # After first iteration the in_dim = gcn_dim
      with tf.variable_scope('%s-%d' % (name,layer)):

        act_sum = tf.zeros([batch_size, max_nodes, gcn_dim])

        w_in   = tf.get_variable('w_in',   [in_dim, gcn_dim],   initializer=tf.contrib.layers.xavier_initializer(),   regularizer=self.regularizer)
        w_out  = tf.get_variable('w_out',  [in_dim, gcn_dim],   initializer=tf.contrib.layers.xavier_initializer(),   regularizer=self.regularizer)
        w_loop = tf.get_variable('w_loop', [in_dim, gcn_dim],   initializer=tf.contrib.layers.xavier_initializer(),   regularizer=self.regularizer)
	 
        #for code optimisation only 
        pre_com_o_in = tf.tensordot(gcn_in, w_in, axes=[[2],[0]]) 
        pre_com_o_out = tf.tensordot(gcn_in, w_out, axes=[[2],[0]])
        pre_com_o_loop = tf.tensordot(gcn_in, w_loop, axes=[[2],[0]])
       

	if use_gating:
          w_gin  = tf.get_variable('w_gin',  [in_dim, 1],   initializer=tf.contrib.layers.xavier_initializer(),   regularizer=self.regularizer)
          w_gout = tf.get_variable('w_gout', [in_dim, 1],   initializer=tf.contrib.layers.xavier_initializer(),   regularizer=self.regularizer)           
          w_gloop = tf.get_variable('w_gloop',[in_dim, 1],  initializer=tf.contrib.layers.xavier_initializer(),   regularizer=self.regularizer)
	
	#for code optimisation only
	  pre_com_o_gin = tf.tensordot(gcn_in, w_gin, axes=[[2],[0]]) 
          pre_com_o_gout = tf.tensordot(gcn_in, w_gout, axes=[[2],[0]])
	  pre_com_o_gloop = tf.tensordot(gcn_in, w_gloop, axes=[[2],[0]])		
	
        
        for lbl in range(max_labels):

          with tf.variable_scope('label-%d_name-%s_layer-%d' % (lbl, name, layer)) as scope:

            #w_in   = tf.get_variable('w_in',   [in_dim, gcn_dim],   initializer=tf.contrib.layers.xavier_initializer(),   regularizer=self.regularizer)
            b_in   = tf.get_variable('b_in',   [1, gcn_dim],    initializer=tf.constant_initializer(0.0),     regularizer=self.regularizer)

            #w_out  = tf.get_variable('w_out',  [in_dim, gcn_dim],   initializer=tf.contrib.layers.xavier_initializer(),   regularizer=self.regularizer)
            b_out  = tf.get_variable('b_out',  [1, gcn_dim],    initializer=tf.constant_initializer(0.0),     regularizer=self.regularizer)

            #w_loop = tf.get_variable('w_loop', [in_dim, gcn_dim],   initializer=tf.contrib.layers.xavier_initializer(),   regularizer=self.regularizer)

            if use_gating:
              #w_gin  = tf.get_variable('w_gin',  [in_dim, 1],   initializer=tf.contrib.layers.xavier_initializer(),   regularizer=self.regularizer)
              b_gin  = tf.get_variable('b_gin',  [1],       initializer=tf.constant_initializer(0.0),     regularizer=self.regularizer)

              #w_gout = tf.get_variable('w_gout', [in_dim, 1],   initializer=tf.contrib.layers.xavier_initializer(),   regularizer=self.regularizer)
              b_gout = tf.get_variable('b_gout', [1],       initializer=tf.constant_initializer(0.0),     regularizer=self.regularizer)

              #w_gloop = tf.get_variable('w_gloop',[in_dim, 1],  initializer=tf.contrib.layers.xavier_initializer(),   regularizer=self.regularizer)

		
		
          with tf.name_scope('in_arcs-%s_name-%s_layer-%d' % (lbl, name, layer)):
            inp_in  = pre_com_o_in + tf.expand_dims(b_in, axis=0)#syntax changed for version compatibility
            in_t    = tf.stack([tf.sparse_tensor_dense_matmul(adj_in[i][lbl], inp_in[i]) for i in range(batch_size)])
      	
            if dropout != 1.0: in_t    = tf.nn.dropout(in_t, keep_prob=dropout)
	    	
            if use_gating:
              inp_gin = pre_com_o_gin + tf.expand_dims(b_gin, axis=0)
              in_gate = tf.stack([tf.sparse_tensor_dense_matmul(adj_in[i][lbl], inp_gin[i]) for i in range(batch_size)])
              in_gsig = tf.sigmoid(in_gate)
              in_act   = in_t * in_gsig
            else:
              in_act   = in_t

 	

          with tf.name_scope('out_arcs-%s_name-%s_layer-%d' % (lbl, name, layer)):
            inp_out  = pre_com_o_out + tf.expand_dims(b_out, axis=0)
            out_t    = tf.stack([tf.sparse_tensor_dense_matmul(adj_out[i][lbl], inp_out[i]) for i in range(batch_size)])
            if dropout != 1.0: out_t    = tf.nn.dropout(out_t, keep_prob=dropout)

            if use_gating:
              inp_gout = pre_com_o_gout + tf.expand_dims(b_gout, axis=0)
              out_gate = tf.stack([tf.sparse_tensor_dense_matmul(adj_out[i][lbl], inp_gout[i]) for i in range(batch_size)])
              out_gsig = tf.sigmoid(out_gate)
              out_act  = out_t * out_gsig
            else:
              out_act = out_t

          with tf.name_scope('self_loop'):
            inp_loop  = pre_com_o_loop
            if dropout != 1.0: inp_loop  = tf.nn.dropout(inp_loop, keep_prob=dropout)

            if use_gating:
              inp_gloop = pre_com_o_gloop
              loop_gsig = tf.sigmoid(inp_gloop)
              loop_act  = inp_loop * loop_gsig
            else:
              loop_act = inp_loop


          act_sum += in_act + out_act + loop_act
        gcn_out = tf.nn.relu(act_sum)
        out.append(gcn_out)

    return gcn_out 

  


  def _reduce_states(self, fw_st, bw_st):
    """Add to the graph a linear layer to reduce the encoder's final FW and BW state into a single initial state for the decoder. This is needed because the encoder is bidirectional but the decoder is not.

    Args:
      fw_st: LSTMStateTuple with hidden_dim units.
      bw_st: LSTMStateTuple with hidden_dim units.

    Returns:
      state: LSTMStateTuple with hidden_dim units.
    """
    hidden_dim = self._hps.hidden_dim
    with tf.variable_scope('reduce_final_st'):

      # Define weights and biases to reduce the cell and reduce the state
      w_reduce_c = tf.get_variable('w_reduce_c', [hidden_dim * 2, hidden_dim], dtype=tf.float32, initializer=self.trunc_norm_init)
      w_reduce_h = tf.get_variable('w_reduce_h', [hidden_dim * 2, hidden_dim], dtype=tf.float32, initializer=self.trunc_norm_init)
      bias_reduce_c = tf.get_variable('bias_reduce_c', [hidden_dim], dtype=tf.float32, initializer=self.trunc_norm_init)
      bias_reduce_h = tf.get_variable('bias_reduce_h', [hidden_dim], dtype=tf.float32, initializer=self.trunc_norm_init)

      # Apply linear layer
      old_c = tf.concat(axis=1, values=[fw_st.c, bw_st.c]) # Concatenation of fw and bw cell
      old_h = tf.concat(axis=1, values=[fw_st.h, bw_st.h]) # Concatenation of fw and bw state
      new_c = tf.nn.relu(tf.matmul(old_c, w_reduce_c) + bias_reduce_c) # Get new cell from old cell
      new_h = tf.nn.relu(tf.matmul(old_h, w_reduce_h) + bias_reduce_h) # Get new state from old state
      return tf.contrib.rnn.LSTMStateTuple(new_c, new_h) # Return new cell and state


  def _add_decoder(self, inputs):
    """Add attention decoder to the graph. In train or eval mode, you call this once to get output on ALL steps. In decode (beam search) mode, you call this once for EACH decoder step.

    Args:
      inputs: inputs to the decoder (word embeddings). A list of tensors shape (batch_size, emb_dim)

    Returns:
      outputs: List of tensors; the outputs of the decoder
      out_state: The final state of the decoder
      attn_dists: A list of tensors; the attention distributions
