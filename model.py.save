# Copyright 2016 The TensorFlow Authors. All Rights Reserved.
# Modifications Copyright 2017 Abigail See
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""This file contains code to build and run the tensorflow graph for the sequence-to-sequence model"""

import os
import time
import numpy as np
import tensorflow as tf
from attention_decoder import attention_decoder
from tensorflow.contrib.tensorboard.plugins import projector
from tensorflow.python.util import nest

_state_size_with_prefix = tf.nn.rnn_cell._state_size_with_prefix

FLAGS = tf.app.flags.FLAGS

def get_initial_cell_state(cell, initializer, batch_size, dtype):
  """Return state tensor(s), initialized with initializer.
  Args:
    cell: RNNCell.
    batch_size: int, float, or unit Tensor representing the batch size.
    initializer: function with two arguments, shape and dtype, that
        determines how the state is initialized.
    dtype: the data type to use for the state.
  Returns:
    If `state_size` is an int or TensorShape, then the return value is a
    `N-D` tensor of shape `[batch_size x state_size]` initialized
    according to the initializer.
    If `state_size` is a nested list or tuple, then the return value is
    a nested list or tuple (of the same structure) of `2-D` tensors with
  the shapes `[batch_size x s]` for each s in `state_size`.
  Snippet from : https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html
  """
  state_size = cell.state_size
  if nest.is_sequence(state_size):
      state_size_flat = nest.flatten(state_size)
      init_state_flat = [
          initializer(_state_size_with_prefix(s), batch_size, dtype, i)
              for i, s in enumerate(state_size_flat)]
      init_state = nest.pack_sequence_as(structure=state_size,
                                  flat_sequence=init_state_flat)
  else:
      init_state_size = _state_size_with_prefix(state_size)
      init_state = initializer(init_state_size, batch_size, dtype, None)

  return init_state

def make_variable_state_initializer(**kwargs):
  def variable_state_initializer(shape, batch_size, dtype, index):
    args = kwargs.copy()

    if args.get('name'):
        args['name'] = args['name'] + '_' + str(index)
    else:
        args['name'] = 'init_state_' + str(index)

    args['shape'] = shape
    args['dtype'] = dtype

    var = tf.get_variable(**args)
    var = tf.expand_dims(var, 0)
    var = tf.tile(var, tf.pack([batch_size] + [1] * len(shape)))
    var.set_shape(_state_size_with_prefix(shape, prefix=[None]))
    return var

  return variable_state_initializer



class SummarizationModel(object):
  """A class to represent a sequence-to-sequence model for text summarization. Supports both baseline mode, pointer-generator mode, and coverage"""

  def __init__(self, hps, vocab):
    self._hps = hps
    self._vocab = vocab
    self.regularizer = None

  def _add_placeholders(self):
    """Add placeholders to the graph. These are entry points for any input data."""
    hps = self._hps

    # encoder part
    self._enc_batch = tf.placeholder(tf.int32, [hps.batch_size, None], name='enc_batch')
    self._enc_lens = tf.placeholder(tf.int32, [hps.batch_size], name='enc_lens')
    self._enc_padding_mask = tf.placeholder(tf.float32, [hps.batch_size, None], name='enc_padding_mask')
    if FLAGS.pointer_gen:
      self._enc_batch_extend_vocab = tf.placeholder(tf.int32, [hps.batch_size, None], name='enc_batch_extend_vocab')
      self._max_art_oovs = tf.placeholder(tf.int32, [], name='max_a# Copyright 2016 The TensorFlow Authors. All Rights Reserved.
# Modifications Copyright 2017 Abigail See
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""This file contains code to build and run the tensorflow graph for the sequence-to-sequence model"""

import os
import time
import numpy as np
import tensorflow as tf
from attention_decoder import attention_decoder
from tensorflow.contrib.tensorboard.plugins import projector
from tensorflow.python.util import nest

_state_size_with_prefix = tf.nn.rnn_cell._state_size_with_prefix

FLAGS = tf.app.flags.FLAGS

def get_initial_cell_state(cell, initializer, batch_size, dtype):
  """Return state tensor(s), initialized with initializer.
  Args:
    cell: RNNCell.
    batch_size: int, float, or unit Tensor representing the batch size.
    initializer: function with two arguments, shape and dtype, that
        determines how the state is initialized.
    dtype: the data type to use for the state.
  Returns:
    If `state_size` is an int or TensorShape, then the return value is a
    `N-D` tensor of shape `[batch_size x state_size]` initialized
    according to the initializer.
    If `state_size` is a nested list or tuple, then the return value is
    a nested list or tuple (of the same structure) of `2-D` tensors with
  the shapes `[batch_size x s]` for each s in `state_size`.
  Snippet from : https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html
  """
  state_size = cell.state_size
  if nest.is_sequence(state_size):
      state_size_flat = nest.flatten(state_size)
      init_state_flat = [
          initializer(_state_size_with_prefix(s), batch_size, dtype, i)
              for i, s in enumerate(state_size_flat)]
      init_state = nest.pack_sequence_as(structure=state_size,
                                  flat_sequence=init_state_flat)
  else:
      init_state_size = _state_size_with_prefix(state_size)
      init_state = initializer(init_state_size, batch_size, dtype, None)

  return init_state

def make_variable_state_initializer(**kwargs):
  def variable_state_initializer(shape, batch_size, dtype, index):
    args = kwargs.copy()

    if args.get('name'):
        args['name'] = args['name'] + '_' + str(index)
    else:
        args['name'] = 'init_state_' + str(index)

    args['shape'] = shape
    args['dtype'] = dtype

    var = tf.get_variable(**args)
    var = tf.expand_dims(var, 0)
    var = tf.tile(var, tf.pack([batch_size] + [1] * len(shape)))
    var.set_shape(_state_size_with_prefix(shape, prefix=[None]))
    return var

  return variable_state_initializer



class SummarizationModel(object):
  """A class to represent a sequence-to-sequence model for text summarization. Supports both baseline mode, pointer-generator mode, and coverage"""

  def __init__(self, hps, vocab):
    self._hps = hps
    self._vocab = vocab
    self.regularizer = None

  def _add_placeholders(self):
    """Add placeholders to the graph. These are entry points for any input data."""
    hps = self._hps

    # encoder part
    self._enc_batch = tf.placeholder(tf.int32, [hps.batch_size, None], name='enc_batch')
    self._enc_lens = tf.placeholder(tf.int32, [hps.batch_size], name='enc_lens')
    self._enc_padding_mask = tf.placeholder(tf.float32, [hps.batch_size, None], name='enc_padding_mask')
    if FLAGS.pointer_gen:
      self._enc_batch_extend_vocab = tf.placeholder(tf.int32, [hps.batch_size, None], name='enc_batch_extend_vocab')
      self._max_art_oovs = tf.placeholder(tf.int32, [], name='max_a